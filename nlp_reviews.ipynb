{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _URL = \"\"\n",
    "\n",
    "# data = tf.keras.utils.get_file(_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0  So there is no way for me to plug it in here i...          0\n",
       "1                         Good case Excellent value.          1\n",
       "2                             Great for the jawbone.          1\n",
       "3  Tied to charger for conversations lasting more...          0\n",
       "4                                  The mic is great.          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/combined_data.csv', index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = data['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_labels = data['sentiment'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##split in test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1594, 398)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_index = int(np.ceil(len(reviews)*0.8))\n",
    "\n",
    "train_reviews = reviews[0:split_index]\n",
    "test_reviews = reviews[split_index: ]\n",
    "\n",
    "len(train_reviews), len(test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1594, 398)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = review_labels[0 : split_index]\n",
    "test_labels = review_labels[split_index : ]\n",
    "\n",
    "len(train_labels), len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make numpy arrays\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels  = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 500\n",
    "embedding_dim = 16\n",
    "max_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token='<OOV>', num_words=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_index:  2834\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print ('word_index: ' , len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sequence of Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 26,  68,   7,  63, 173,  13,  67,   8, 219,   5,  16,  82,  16,\n",
       "         2, 198, 364,   4,  75, 109,   6,   1,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_padded_sequences = pad_sequences(train_sequences, padding='post', maxlen=max_length, truncating='post')\n",
    "train_padded_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_padded_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'So there is no way for me to plug it in here in the US unless I go by a converter.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26,\n",
       " 68,\n",
       " 7,\n",
       " 63,\n",
       " 173,\n",
       " 13,\n",
       " 67,\n",
       " 8,\n",
       " 219,\n",
       " 5,\n",
       " 16,\n",
       " 82,\n",
       " 16,\n",
       " 2,\n",
       " 198,\n",
       " 364,\n",
       " 4,\n",
       " 75,\n",
       " 109,\n",
       " 6,\n",
       " 1]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_reviews)\n",
    "test_padded_sequences = pad_sequences(test_sequences, maxlen = max_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "l0 = tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_length)\n",
    "l1 = tf.keras.layers.Flatten()\n",
    "\n",
    "l2 = tf.keras.layers.Dense(units=6, activation='relu')\n",
    "\n",
    "l3 = tf.keras.layers.Dense(units=2, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([l0, l1, l2, l3])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 16)           8000      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 9606      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 14        \n",
      "=================================================================\n",
      "Total params: 17,620\n",
      "Trainable params: 17,620\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6918 - accuracy: 0.5270 - val_loss: 0.6955 - val_accuracy: 0.4221\n",
      "Epoch 2/12\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.6798 - accuracy: 0.5659 - val_loss: 0.6749 - val_accuracy: 0.6658\n",
      "Epoch 3/12\n",
      "50/50 [==============================] - 0s 965us/step - loss: 0.6339 - accuracy: 0.7026 - val_loss: 0.6303 - val_accuracy: 0.6859\n",
      "Epoch 4/12\n",
      "50/50 [==============================] - 0s 974us/step - loss: 0.5240 - accuracy: 0.8099 - val_loss: 0.5598 - val_accuracy: 0.7387\n",
      "Epoch 5/12\n",
      "50/50 [==============================] - 0s 980us/step - loss: 0.4036 - accuracy: 0.8607 - val_loss: 0.5077 - val_accuracy: 0.7613\n",
      "Epoch 6/12\n",
      "50/50 [==============================] - 0s 986us/step - loss: 0.3189 - accuracy: 0.8927 - val_loss: 0.5044 - val_accuracy: 0.7362\n",
      "Epoch 7/12\n",
      "50/50 [==============================] - 0s 995us/step - loss: 0.2631 - accuracy: 0.9090 - val_loss: 0.5017 - val_accuracy: 0.7487\n",
      "Epoch 8/12\n",
      "50/50 [==============================] - 0s 977us/step - loss: 0.2193 - accuracy: 0.9304 - val_loss: 0.4913 - val_accuracy: 0.7487\n",
      "Epoch 9/12\n",
      "50/50 [==============================] - 0s 985us/step - loss: 0.1852 - accuracy: 0.9454 - val_loss: 0.4942 - val_accuracy: 0.7563\n",
      "Epoch 10/12\n",
      "50/50 [==============================] - 0s 1000us/step - loss: 0.1587 - accuracy: 0.9598 - val_loss: 0.5326 - val_accuracy: 0.7387\n",
      "Epoch 11/12\n",
      "50/50 [==============================] - 0s 979us/step - loss: 0.1405 - accuracy: 0.9630 - val_loss: 0.5376 - val_accuracy: 0.7563\n",
      "Epoch 12/12\n",
      "50/50 [==============================] - 0s 998us/step - loss: 0.1226 - accuracy: 0.9668 - val_loss: 0.5450 - val_accuracy: 0.7638\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd3a2980520>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 12\n",
    "model.fit(train_padded_sequences,\n",
    "          train_labels,\n",
    "          validation_data=(test_padded_sequences,test_labels),\n",
    "          epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuple_list = [(value, key) for (key, value) in word_index.items()]\n",
    "\n",
    "# reverse_word_index = dict(tuple_list)\n",
    "\n",
    "reverse_word_index  = {}\n",
    "for key, value in word_index.items():\n",
    "    reverse_word_index[value] = key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 16)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddig_weights = l0.get_weights()[0]\n",
    "embeddig_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# embedding vectors and meta-data\n",
    "out_v = io.open('vec.tsv', 'w', encoding='utf-8')  # contains weigh vector of each word\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8') # contain word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-0.013115052\\t-0.03802334\\t-0.029480202\\t-0.014087418\\t0.0044238367\\t0.07983513\\t-0.04049583\\t0.036401916\\t0.010341115\\t0.0065676398\\t0.089937516\\t0.03852202\\t0.0013121986\\t-0.04855585\\t0.03795019\\t-0.061724033'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tab delimited \n",
    "'\\t'.join([str(x) for x in embeddig_weights[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word_num in range(1, vocab_size):\n",
    "    word = reverse_word_index[word_num]\n",
    "    embedding = embeddig_weights[word_num]\n",
    "    \n",
    "    out_m.write(word+\"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in embeddig_weights[1]]) + \"\\n\")\n",
    "\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I love chocolate',\n",
       " 'This restaurant sucks',\n",
       " 'OMG, what a shake!',\n",
       " 'Totally recommend this place for grilled sandwitches',\n",
       " 'I would lose my license instead of standing in long lines',\n",
       " 'please never come to this fish market, it smells too bad']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_reviews =['I love chocolate', \n",
    "              'This restaurant sucks',\n",
    "              'OMG, what a shake!', \n",
    "              'Totally recommend this place for grilled sandwitches', \n",
    "              'I would lose my license instead of standing in long lines', \n",
    "               'please never come to this fish market, it smells too bad']\n",
    "\n",
    "fake_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_seq = tokenizer.texts_to_sequences(fake_reviews)\n",
    "fake_pad_seq = pad_sequences(fake_seq, maxlen=max_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = model.predict(fake_pad_seq)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love chocolate\n",
      "[0.00175381 0.9982462 ]\n",
      "\n",
      "\n",
      "This restaurant sucks\n",
      "[0.7365322  0.26346776]\n",
      "\n",
      "\n",
      "OMG, what a shake!\n",
      "[0.23255609 0.76744384]\n",
      "\n",
      "\n",
      "Totally recommend this place for grilled sandwitches\n",
      "[0.02220836 0.9777916 ]\n",
      "\n",
      "\n",
      "I would lose my license instead of standing in long lines\n",
      "[0.16695921 0.8330408 ]\n",
      "\n",
      "\n",
      "please never come to this fish market, it smells too bad\n",
      "[0.9921681  0.00783186]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(fake_reviews)):\n",
    "    print(fake_reviews[i])\n",
    "    print(classes[i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-tf2]",
   "language": "python",
   "name": "conda-env-py3-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
