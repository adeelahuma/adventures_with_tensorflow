{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Tensorflow \n",
    "\n",
    "scalar(rank: 0)  --> vector (rank: 1)--> matrices(rank: 2) --> tensor \n",
    "\n",
    "- Two popular machine learning libraries :\n",
    "    - sklearn; can be good for kmeans clustering , random forest  \n",
    "    - tensorflow; google released in end of 2015; leading library for neural networks, cnn and rnn\n",
    "        - uses both cpu and gpu; this is crucial for the speed of algorithms and TF utilizes more computing power and this is done automatically \n",
    "        - google also introduced TPU (Tebsor Processing unit)\n",
    "        \n",
    "Tensor flow is a **deep learning library**     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Tensorflow 1 vs Tensorflow 2\n",
    "- TF1 strange methods and logic of code is unlike other libraries and hard to learn and use. \n",
    "- This led to development of high level pacakges such as PyTorch and Keras\n",
    "- in 2015 Keras was integrated in core TF\n",
    "- Keras and TF are open source \n",
    "- Keras is conceived as an interface for TF rather than a different libraries.\n",
    "- TF2 came in 2019\n",
    "    - it borrowed keras syntax\n",
    "    - no duplicate or deprecated functions\n",
    "    - eager execution (?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF file formats\n",
    "- TF does not work well with excel and csv files\n",
    "- Its tensor based , so **we want format that can store the information in tensors** \n",
    "- .npz files \n",
    "    - numpy's file type. It allows to stores n-dimensional arrays\n",
    "    - tensors can be represented as n-dimensional arrays. in npz files data is already organized in required format \n",
    "    - in deep learning: data --> preprocess --. save as .npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 2) (100000, 1)\n"
     ]
    }
   ],
   "source": [
    "n = 100000 #\n",
    "\n",
    "x = np.random.uniform(low=-10,high=10, size=(n, 1))\n",
    "z = np.random.uniform(-10,10, (n, 1))\n",
    "\n",
    "inputs = np.column_stack((x,z))\n",
    "\n",
    "\n",
    "noise = np.random.uniform(-1,1, (n,1))\n",
    "\n",
    "y  = 2*x - 3*z + 5 + noise\n",
    "inputs.shape\n",
    "\n",
    "print(inputs.shape , y.shape)\n",
    "\n",
    "# saving in TF format \n",
    "np.savez('tf_intro', inputs= inputs, targets = y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving with TF \n",
    "- With TF we must build our model \n",
    "- tf.keras --> module\n",
    "- tf.keras.Sequential  --> function that indicates that we are laying down the model \n",
    "- tf.keras.layers.Dense --> takes the input provided to the model and calculates the dot product of the inputs and the weights and adds the bias. Also applies activation function \n",
    "- SGD --> Stochastic Gradient Descent; generalization of gradient descent algorithm \n",
    "- Epoch --> iteration over the full dataset \n",
    "- verbose = 0 stands for silent or no output about the training is displayed\n",
    "- try verbose = 1 and verbose = 2 \n",
    "- kernel_initializer --> broader term for weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.load('tf_intro.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3125/3125 - 1s - loss: 0.6149\n",
      "Epoch 2/100\n",
      "3125/3125 - 1s - loss: 0.3805\n",
      "Epoch 3/100\n",
      "3125/3125 - 1s - loss: 0.3820\n",
      "Epoch 4/100\n",
      "3125/3125 - 1s - loss: 0.3810\n",
      "Epoch 5/100\n",
      "3125/3125 - 1s - loss: 0.3816\n",
      "Epoch 6/100\n",
      "3125/3125 - 1s - loss: 0.3829\n",
      "Epoch 7/100\n",
      "3125/3125 - 1s - loss: 0.3804\n",
      "Epoch 8/100\n",
      "3125/3125 - 1s - loss: 0.3790\n",
      "Epoch 9/100\n",
      "3125/3125 - 1s - loss: 0.3818\n",
      "Epoch 10/100\n",
      "3125/3125 - 1s - loss: 0.3787\n",
      "Epoch 11/100\n",
      "3125/3125 - 1s - loss: 0.3804\n",
      "Epoch 12/100\n",
      "3125/3125 - 1s - loss: 0.3792\n",
      "Epoch 13/100\n",
      "3125/3125 - 1s - loss: 0.3799\n",
      "Epoch 14/100\n",
      "3125/3125 - 1s - loss: 0.3817\n",
      "Epoch 15/100\n",
      "3125/3125 - 1s - loss: 0.3809\n",
      "Epoch 16/100\n",
      "3125/3125 - 1s - loss: 0.3798\n",
      "Epoch 17/100\n",
      "3125/3125 - 1s - loss: 0.3785\n",
      "Epoch 18/100\n",
      "3125/3125 - 1s - loss: 0.3785\n",
      "Epoch 19/100\n",
      "3125/3125 - 1s - loss: 0.3809\n",
      "Epoch 20/100\n",
      "3125/3125 - 1s - loss: 0.3811\n",
      "Epoch 21/100\n",
      "3125/3125 - 1s - loss: 0.3770\n",
      "Epoch 22/100\n",
      "3125/3125 - 1s - loss: 0.3800\n",
      "Epoch 23/100\n",
      "3125/3125 - 1s - loss: 0.3797\n",
      "Epoch 24/100\n",
      "3125/3125 - 1s - loss: 0.3832\n",
      "Epoch 25/100\n",
      "3125/3125 - 1s - loss: 0.3851\n",
      "Epoch 26/100\n",
      "3125/3125 - 1s - loss: 0.3801\n",
      "Epoch 27/100\n",
      "3125/3125 - 1s - loss: 0.3783\n",
      "Epoch 28/100\n",
      "3125/3125 - 1s - loss: 0.3802\n",
      "Epoch 29/100\n",
      "3125/3125 - 1s - loss: 0.3807\n",
      "Epoch 30/100\n",
      "3125/3125 - 1s - loss: 0.3805\n",
      "Epoch 31/100\n",
      "3125/3125 - 1s - loss: 0.3780\n",
      "Epoch 32/100\n",
      "3125/3125 - 1s - loss: 0.3815\n",
      "Epoch 33/100\n",
      "3125/3125 - 1s - loss: 0.3788\n",
      "Epoch 34/100\n",
      "3125/3125 - 1s - loss: 0.3786\n",
      "Epoch 35/100\n",
      "3125/3125 - 1s - loss: 0.3786\n",
      "Epoch 36/100\n",
      "3125/3125 - 1s - loss: 0.3803\n",
      "Epoch 37/100\n",
      "3125/3125 - 1s - loss: 0.3779\n",
      "Epoch 38/100\n",
      "3125/3125 - 1s - loss: 0.3794\n",
      "Epoch 39/100\n",
      "3125/3125 - 1s - loss: 0.3806\n",
      "Epoch 40/100\n",
      "3125/3125 - 1s - loss: 0.3796\n",
      "Epoch 41/100\n",
      "3125/3125 - 1s - loss: 0.3799\n",
      "Epoch 42/100\n",
      "3125/3125 - 1s - loss: 0.3817\n",
      "Epoch 43/100\n",
      "3125/3125 - 1s - loss: 0.3795\n",
      "Epoch 44/100\n",
      "3125/3125 - 1s - loss: 0.3847\n",
      "Epoch 45/100\n",
      "3125/3125 - 1s - loss: 0.3824\n",
      "Epoch 46/100\n",
      "3125/3125 - 1s - loss: 0.3791\n",
      "Epoch 47/100\n",
      "3125/3125 - 1s - loss: 0.3785\n",
      "Epoch 48/100\n",
      "3125/3125 - 1s - loss: 0.3802\n",
      "Epoch 49/100\n",
      "3125/3125 - 1s - loss: 0.3825\n",
      "Epoch 50/100\n",
      "3125/3125 - 1s - loss: 0.3825\n",
      "Epoch 51/100\n",
      "3125/3125 - 1s - loss: 0.3788\n",
      "Epoch 52/100\n",
      "3125/3125 - 1s - loss: 0.3804\n",
      "Epoch 53/100\n",
      "3125/3125 - 1s - loss: 0.3789\n",
      "Epoch 54/100\n",
      "3125/3125 - 1s - loss: 0.3802\n",
      "Epoch 55/100\n",
      "3125/3125 - 1s - loss: 0.3787\n",
      "Epoch 56/100\n",
      "3125/3125 - 1s - loss: 0.3814\n",
      "Epoch 57/100\n",
      "3125/3125 - 1s - loss: 0.3801\n",
      "Epoch 58/100\n",
      "3125/3125 - 1s - loss: 0.3830\n",
      "Epoch 59/100\n",
      "3125/3125 - 1s - loss: 0.3822\n",
      "Epoch 60/100\n",
      "3125/3125 - 1s - loss: 0.3821\n",
      "Epoch 61/100\n",
      "3125/3125 - 1s - loss: 0.3815\n",
      "Epoch 62/100\n",
      "3125/3125 - 1s - loss: 0.3807\n",
      "Epoch 63/100\n",
      "3125/3125 - 1s - loss: 0.3807\n",
      "Epoch 64/100\n",
      "3125/3125 - 1s - loss: 0.3810\n",
      "Epoch 65/100\n",
      "3125/3125 - 1s - loss: 0.3798\n",
      "Epoch 66/100\n",
      "3125/3125 - 1s - loss: 0.3834\n",
      "Epoch 67/100\n",
      "3125/3125 - 1s - loss: 0.3788\n",
      "Epoch 68/100\n",
      "3125/3125 - 1s - loss: 0.3810\n",
      "Epoch 69/100\n",
      "3125/3125 - 1s - loss: 0.3802\n",
      "Epoch 70/100\n",
      "3125/3125 - 2s - loss: 0.3762\n",
      "Epoch 71/100\n",
      "3125/3125 - 2s - loss: 0.3799\n",
      "Epoch 72/100\n",
      "3125/3125 - 1s - loss: 0.3850\n",
      "Epoch 73/100\n",
      "3125/3125 - 1s - loss: 0.3779\n",
      "Epoch 74/100\n",
      "3125/3125 - 1s - loss: 0.3803\n",
      "Epoch 75/100\n",
      "3125/3125 - 1s - loss: 0.3820\n",
      "Epoch 76/100\n",
      "3125/3125 - 1s - loss: 0.3818\n",
      "Epoch 77/100\n",
      "3125/3125 - 1s - loss: 0.3817\n",
      "Epoch 78/100\n",
      "3125/3125 - 1s - loss: 0.3837\n",
      "Epoch 79/100\n",
      "3125/3125 - 1s - loss: 0.3799\n",
      "Epoch 80/100\n",
      "3125/3125 - 1s - loss: 0.3807\n",
      "Epoch 81/100\n",
      "3125/3125 - 1s - loss: 0.3801\n",
      "Epoch 82/100\n",
      "3125/3125 - 1s - loss: 0.3816\n",
      "Epoch 83/100\n",
      "3125/3125 - 1s - loss: 0.3826\n",
      "Epoch 84/100\n",
      "3125/3125 - 1s - loss: 0.3782\n",
      "Epoch 85/100\n",
      "3125/3125 - 1s - loss: 0.3805\n",
      "Epoch 86/100\n",
      "3125/3125 - 1s - loss: 0.3780\n",
      "Epoch 87/100\n",
      "3125/3125 - 1s - loss: 0.3819\n",
      "Epoch 88/100\n",
      "3125/3125 - 1s - loss: 0.3786\n",
      "Epoch 89/100\n",
      "3125/3125 - 1s - loss: 0.3805\n",
      "Epoch 90/100\n",
      "3125/3125 - 1s - loss: 0.3800\n",
      "Epoch 91/100\n",
      "3125/3125 - 1s - loss: 0.3803\n",
      "Epoch 92/100\n",
      "3125/3125 - 1s - loss: 0.3765\n",
      "Epoch 93/100\n",
      "3125/3125 - 1s - loss: 0.3813\n",
      "Epoch 94/100\n",
      "3125/3125 - 1s - loss: 0.3813\n",
      "Epoch 95/100\n",
      "3125/3125 - 1s - loss: 0.3820\n",
      "Epoch 96/100\n",
      "3125/3125 - 1s - loss: 0.3812\n",
      "Epoch 97/100\n",
      "3125/3125 - 1s - loss: 0.3860\n",
      "Epoch 98/100\n",
      "3125/3125 - 1s - loss: 0.3797\n",
      "Epoch 99/100\n",
      "3125/3125 - 1s - loss: 0.3774\n",
      "Epoch 100/100\n",
      "3125/3125 - 1s - loss: 0.3809\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8b99784190>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 2\n",
    "output_size = 1\n",
    "\n",
    "model  = tf.keras.Sequential([\n",
    "                                tf.keras.layers.Dense(output_size, \n",
    "                                kernel_initializer = tf.random_uniform_initializer(minval=-0.1, maxval=0.1),\n",
    "                                bias_initializer = tf.random_uniform_initializer(minval=-0.1, maxval=0.1)) \n",
    "                                \n",
    "                            ])\n",
    "\n",
    "custom_optimizer = tf.keras.optimizers.SGD(learning_rate=0.02)\n",
    "\n",
    "model.compile(optimizer = custom_optimizer, loss = 'mean_squared_error')\n",
    "\n",
    "\n",
    "model.fit(training_data['inputs'], training_data['targets'], epochs = 100, verbose = 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract weights and biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.9896656],\n",
       "        [-3.0432231]], dtype=float32),\n",
       " array([4.9887834], dtype=float32)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.9896656],\n",
       "       [-3.0432231]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.layers[0].get_weights()[0]\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.9887834], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias = model.layers[0].get_weights()[1]\n",
    "bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the outputs (make predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -8.9],\n",
       "       [ 13.3],\n",
       "       [-12.3],\n",
       "       ...,\n",
       "       [ 14.7],\n",
       "       [ 22.7],\n",
       "       [ 12.4]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_on_batch(training_data['inputs']).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -7.9],\n",
       "       [ 13.4],\n",
       "       [-12.3],\n",
       "       ...,\n",
       "       [ 15.7],\n",
       "       [ 23.1],\n",
       "       [ 11.5]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data['targets'].round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_data['targets'], model.predict_on_batch(training_data['inputs']))\n",
    "plt.xlabel('y')\n",
    "plt.ylabel('y_hat')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.squeeze?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "solve the following exercises\n",
    "\n",
    "    1. Change the number of observations to 100,000 and see what happens. \n",
    "    >> each epoch took 1s second; loss oscillated between 0.3787 and 0.3804\n",
    "\n",
    "    2. Play around with the learning rate. Values like 0.0001, 0.001, 0.1, 1 are all interesting to observe.\n",
    "\n",
    "    3. Change the loss function. An alternative loss for regressions is the Huber loss.\n",
    "\n",
    "The Huber loss is more appropriate than the L2-norm when we have outliers, as it is less sensitive to them (in our example we don't have outliers, but you will surely stumble upon a dataset with outliers in the future). The L2-norm loss puts all differences *to the square*, so outliers have a lot of influence on the outcome. The proper syntax of the Huber loss is 'huber_loss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-tf2.0]",
   "language": "python",
   "name": "conda-env-py3-tf2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
