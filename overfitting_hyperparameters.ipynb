{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "What's overfitting and how do we deal with it ?\n",
    "\n",
    "- Overfitting \n",
    "    - our training has focused on the particular training set so much, it has 'missed the point'\n",
    "    - random noise is captured in overffited model \n",
    "    - low loss, low accuracy \n",
    "- Underfitting \n",
    "    - the modle has not captured the underlying logic of the data \n",
    "    - high loss; low accuracy \n",
    "    \n",
    "- Good Model\n",
    "    - low loss, high accuracy \n",
    "\n",
    "- Computer program is not wrong, its us who made a mistake. we must keep issues overfitting/underfitting to take precautions. \n",
    "\n",
    "\n",
    "- Memes are interesting way to find if you understand the concept. If you find it funny, then you got it. \n",
    "\n",
    "- Overfitting real enemy in ML \n",
    "- Idnetity Overfitting :\n",
    "    - divide data into train, test, val datasets\n",
    "    - train-set: thats where training happens\n",
    "    - val-set:   help us prevent overfitting \n",
    "    - Red-flag : if training error is getting lower but validation error is getting higher \n",
    "    - test-set: measures the final predictive power of the model; equivalent to applying model to real life\n",
    "    - split : 80-10-10 or 70-20-10\n",
    "    - validate for every epoch \n",
    "- k-fold cross validation \n",
    "    - use if small dataset\n",
    "    - training + val , test \n",
    "    - pros: utilize more data for training, con: possibly overfitted a bit\n",
    "    - better to divide in 3 sets.\n",
    "\n",
    "- Early Stopping: technique to prevent overfitting ; stop early b4 overfitting \n",
    "    -1.  train for preset number of epochs; no gaurentee that the minimum is reached, maybe doesn't minimize at all\n",
    "    - 2. stop when update becomes too small\n",
    "    - 3. validation set strategy:\n",
    "    \n",
    "    - Better :stop when validation loss starts increasing or when training loss becomes very small\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialization: process in which we set the initial values of weights\n",
    "    - If all weights are same, its useless\n",
    "- Types of Initialization    \n",
    "    - uniform randomly initialize \n",
    "    - normal initializer (mean 0 and std =0.1) --> know its issue \n",
    "    - xavier initializatio/glorot initialization \n",
    "        - uniform\n",
    "        - normal\n",
    "     - xavier is default initializer in TF    \n",
    "    \n",
    "- non-linearity are essentials for deep net     \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization : algorithms we use to vary our model's parameters \n",
    "\n",
    "- Grdaient Descent /Batch GD: clumsy optimizer \n",
    "- Stochastic Gradient Descent : updates the weights inside the epoch real time\n",
    "    - related to batching \n",
    "\n",
    "Gradient Descent Pitfalls\n",
    "- each local minimum is the suboptimal solution to the machine learning optimization\n",
    "- it can stuck in local minima\n",
    "\n",
    "\n",
    "GD and SGD \n",
    "- Momentum \n",
    "    - $\\alpha = 0.9$ ; a hyper-parameter\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper Parameters: pre-set by us\n",
    "- width\n",
    "- Depth \n",
    "- Learning rate \n",
    "- btch size\n",
    "- momemntum \n",
    "- decay coefficients\n",
    "\n",
    "Parameters: found by optimizing \n",
    "\n",
    "- weights\n",
    "- biases\n",
    "\n",
    "\n",
    "\n",
    "- Learning Rate Schedules\n",
    "    - strat with high  initial learning rate ; 0.1--> first 5 epochs\n",
    "    - At some point lower the rate to avoid oscillation ; 0.01 --> next 5 epochs\n",
    "    - Around the end we pick a very small rate to get a precise answer; 0.001 --> until the end\n",
    "    \n",
    "    \n",
    "- Exponential Scheudle\n",
    "    - strat with high  initial learning rate ; 0.1\n",
    "    - At some point lower the rate to avoid oscillation ; \n",
    "    - Around the end we pick a very small rate to get a precise answer; 0.001 --> until the end\n",
    "\n",
    "- Adaptive Learning Rate Schedules\n",
    "    - AdaGrad (adaptive gradient algorithm)\n",
    "    - RMSProp (roor mean square propogation)\n",
    "    - ADAM (Adaptive moment estimation\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing : any manipulation of the dataset before running it through the model \n",
    "\n",
    "Why ?\n",
    "- compatibility with the tools\n",
    "- we may need to adjust orders of magnitude\n",
    "- generalization \n",
    "\n",
    "- .csv/.xls must convert to.npz\n",
    "\n",
    "Baisc preprocessing\n",
    "- relative metrics\n",
    "- log tranformations\n",
    "\n",
    "Standardization /Feture Scaling \n",
    "- tranforming data into a standard scale \n",
    "\n",
    "Normalization --> l1 norm vector\n",
    "\n",
    "pca \n",
    "\n",
    "whitening ; done after pca \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical Data \n",
    "\n",
    "- One-hot encoding >>> few categories\n",
    "- Binary encoding  >>> many cateogories\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-tf2.0]",
   "language": "python",
   "name": "conda-env-py3-tf2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
